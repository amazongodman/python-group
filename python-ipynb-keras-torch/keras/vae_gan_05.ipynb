{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-gan-05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "7YlhR71k5ucA",
        "outputId": "32454dbc-847b-4098-f206-b6c3fda7a105"
      },
      "source": [
        "!pip install keras==2.0.8\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.0.8 in /usr/local/lib/python3.6/dist-packages (2.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.4.1)\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 49kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.33.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (50.3.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN3_5dngfvE4"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_znkJhWT5wVl"
      },
      "source": [
        "#vaeをつくる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-XhiBCr5wTK"
      },
      "source": [
        "#vaeは画像の構造的な生成を中間の正規分布が学習するので、笑顔とか男女とかの傾向を学習したりする\n",
        "\n",
        "VAEは構造化を正則化損失により過学習せず学び\n",
        "且つ再構築の復元損失を最小化する\n",
        "\n",
        "ただし構造化を正規分布として仮定しているのでぼやける"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "dyzGIyC75wQk",
        "outputId": "39a6a7e8-b967-431c-c9fd-b355384d7871"
      },
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.0.8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKh5KMLKffQf"
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "KGql7D-T5wNe",
        "outputId": "6146818e-eb05-4157-958c-a167003a8fd1"
      },
      "source": [
        "# Encode the input into a mean and variance parameter\n",
        "z_mean, z_log_variance = encoder(input_img)\n",
        "\n",
        "# Draw a latent point using a small random epsilon\n",
        "z = z_mean + exp(z_log_variance) * epsilon\n",
        "\n",
        "# Then decode z back to an image\n",
        "reconstructed_img = decoder(z)\n",
        "\n",
        "# Instantiate a model\n",
        "model = Model(input_img, reconstructed_img)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b779ac7044d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encode the input into a mean and variance parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Draw a latent point using a small random epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_log_variance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPCEmgET5wK7",
        "outputId": "7cc97f9a-aef5-495d-85c2-ef0e678a6980"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "img_shape = (28, 28, 1)\n",
        "batch_size = 16\n",
        "latent_dim = 2  # Dimensionality of the latent space: a plane\n",
        "\n",
        "input_img = keras.Input(shape=img_shape)\n",
        "\n",
        "x = layers.Conv2D(32, 3,\n",
        "                  padding='same', activation='relu')(input_img)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', activation='relu',\n",
        "                  strides=(2, 2))(x)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', activation='relu')(x)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', activation='relu')(x)\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBnCAEMN5wIa"
      },
      "source": [
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
        "                              mean=0., stddev=1.)\n",
        "    return z_mean + K.exp(z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrMAK9zQ5wFq"
      },
      "source": [
        "z = layers.Lambda(sampling)([z_mean, z_log_var])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nevyYs9y5wDH"
      },
      "source": [
        "# This is the input where we will feed `z`.\n",
        "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
        "\n",
        "# Upsample to the correct number of units\n",
        "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activation='relu')(decoder_input)\n",
        "\n",
        "# Reshape into an image of the same shape as before our last `Flatten` layer\n",
        "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "\n",
        "# We then apply then reverse operation to the initial\n",
        "# stack of convolution layers: a `Conv2DTranspose` layers\n",
        "# with corresponding parameters.\n",
        "x = layers.Conv2DTranspose(32, 3,\n",
        "                           padding='same', activation='relu',\n",
        "                           strides=(2, 2))(x)\n",
        "x = layers.Conv2D(1, 3,\n",
        "                  padding='same', activation='sigmoid')(x)\n",
        "# We end up with a feature map of the same size as the original input.\n",
        "\n",
        "# This is our decoder model.\n",
        "decoder = Model(decoder_input, x)\n",
        "\n",
        "# We then apply it to `z` to recover the decoded `z`.\n",
        "z_decoded = decoder(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPwDXQJ5wAa"
      },
      "source": [
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = K.flatten(x)\n",
        "        z_decoded = K.flatten(z_decoded)\n",
        "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "        kl_loss = -5e-4 * K.mean(\n",
        "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # We don't use this output.\n",
        "        return x\n",
        "\n",
        "# We call our custom layer on the input and the decoded output,\n",
        "# to obtain the final model output.\n",
        "y = CustomVariationalLayer()([input_img, z_decoded])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5I58TgN5v9f",
        "outputId": "41d1dfe4-b2ec-4224-8c4d-28a6657148ac"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "vae = Model(input_img, y)\n",
        "vae.compile(optimizer='rmsprop', loss=None)\n",
        "vae.summary()\n",
        "\n",
        "# Train the VAE on MNIST digits\n",
        "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "\n",
        "vae.fit(x=x_train, y=None,\n",
        "        shuffle=True,\n",
        "        epochs=10,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, None))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "input_1 (InputLayer)             (None, 28, 28, 1)     0                                            \n",
            "____________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)                (None, 28, 28, 32)    320         input_1[0][0]                    \n",
            "____________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)                (None, 14, 14, 64)    18496       conv2d_1[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)                (None, 14, 14, 64)    36928       conv2d_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)                (None, 14, 14, 64)    36928       conv2d_3[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)              (None, 12544)         0           conv2d_4[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "dense_1 (Dense)                  (None, 32)            401440      flatten_1[0][0]                  \n",
            "____________________________________________________________________________________________________\n",
            "dense_2 (Dense)                  (None, 2)             66          dense_1[0][0]                    \n",
            "____________________________________________________________________________________________________\n",
            "dense_3 (Dense)                  (None, 2)             66          dense_1[0][0]                    \n",
            "____________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)                (None, 2)             0           dense_2[0][0]                    \n",
            "                                                                   dense_3[0][0]                    \n",
            "____________________________________________________________________________________________________\n",
            "model_1 (Model)                  (None, 28, 28, 1)     56385       lambda_1[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "custom_variational_layer_1 (Cust [(None, 28, 28, 1), ( 0           input_1[0][0]                    \n",
            "                                                                   model_1[1][0]                    \n",
            "====================================================================================================\n",
            "Total params: 550,629\n",
            "Trainable params: 550,629\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 493s - loss: 21130653012.4707 - val_loss: 0.2032\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 490s - loss: 0.1980 - val_loss: 0.1921\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 495s - loss: 0.1912 - val_loss: 0.1906\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 498s - loss: 0.1883 - val_loss: 0.1871\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 500s - loss: 0.1875 - val_loss: 0.1851\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 492s - loss: 0.1908 - val_loss: 0.1946\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 489s - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 483s - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 484s - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 486s - loss: nan - val_loss: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5a8250320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1_RCsRDz5v7G",
        "outputId": "27540568-0819-4cbf-be79-5b5718d8e23c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
        "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='Greys_r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/image.py:452: UserWarning: Warning: converting a masked element to nan.\n",
            "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/image.py:459: UserWarning: Warning: converting a masked element to nan.\n",
            "  a_min = np.float64(newmin)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/image.py:464: UserWarning: Warning: converting a masked element to nan.\n",
            "  a_max = np.float64(newmax)\n",
            "<string>:6: UserWarning: Warning: converting a masked element to nan.\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:85: UserWarning: Warning: converting a masked element to nan.\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ItCpNq245v4Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tIDCcai85v1f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1mcOmHwgUoO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt3fLRHpgUrM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alj2OkvegUt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vQ2Qok4gUwb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDh4AK2ggUzH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qw_CrFIgU13"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTSJLHT0gU4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLNhM10B5vy4"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "latent_dim = 32\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "\n",
        "generator_input = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "# First, transform the input into a 16x16 128-channels feature map\n",
        "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Reshape((16, 16, 128))(x)\n",
        "\n",
        "# Then, add a convolution layer\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Upsample to 32x32\n",
        "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Few more conv layers\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Produce a 32x32 1-channel feature map\n",
        "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
        "generator = keras.models.Model(generator_input, x)\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PLY-84u5vwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD3ptNFK5vt3"
      },
      "source": [
        "discriminator_input = layers.Input(shape=(height, width, channels))\n",
        "x = layers.Conv2D(128, 3)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# One dropout layer - important trick!\n",
        "x = layers.Dropout(0.4)(x)\n",
        "\n",
        "# Classification layer\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "discriminator = keras.models.Model(discriminator_input, x)\n",
        "discriminator.summary()\n",
        "\n",
        "# To stabilize training, we use learning rate decay\n",
        "# and gradient clipping (by value) in the optimizer.\n",
        "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyKdQGTA5vrS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFkqLBn5vo5"
      },
      "source": [
        "# Set discriminator weights to non-trainable\n",
        "# (will only apply to the `gan` model)\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = keras.models.Model(gan_input, gan_output)\n",
        "\n",
        "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzc5KUZG5vnE"
      },
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Load CIFAR10 data\n",
        "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Select frog images (class 6)\n",
        "x_train = x_train[y_train.flatten() == 6]\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.reshape(\n",
        "    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
        "\n",
        "iterations = 10000\n",
        "batch_size = 20\n",
        "save_dir = '/home/ubuntu/gan_images/'\n",
        "\n",
        "# Start training loop\n",
        "start = 0\n",
        "for step in range(iterations):\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "    # Decode them to fake images\n",
        "    generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "    # Combine them with real images\n",
        "    stop = start + batch_size\n",
        "    real_images = x_train[start: stop]\n",
        "    combined_images = np.concatenate([generated_images, real_images])\n",
        "\n",
        "    # Assemble labels discriminating real from fake images\n",
        "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
        "                             np.zeros((batch_size, 1))])\n",
        "    # Add random noise to the labels - important trick!\n",
        "    labels += 0.05 * np.random.random(labels.shape)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "\n",
        "    # sample random points in the latent space\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "    # Assemble labels that say \"all real images\"\n",
        "    misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the generator (via the gan model,\n",
        "    # where the discriminator weights are frozen)\n",
        "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
        "    \n",
        "    start += batch_size\n",
        "    if start > len(x_train) - batch_size:\n",
        "      start = 0\n",
        "\n",
        "    # Occasionally save / plot\n",
        "    if step % 100 == 0:\n",
        "        # Save model weights\n",
        "        gan.save_weights('gan.h5')\n",
        "\n",
        "        # Print metrics\n",
        "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
        "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
        "\n",
        "        # Save one generated image\n",
        "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
        "\n",
        "        # Save one real image, for comparison\n",
        "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muEGlaMH5vkq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample random points in the latent space\n",
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
        "\n",
        "# Decode them to fake images\n",
        "generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "for i in range(generated_images.shape[0]):\n",
        "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKI3nf3K5vhc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziBSN2sg5vf9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLo_fWQQ5vck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}