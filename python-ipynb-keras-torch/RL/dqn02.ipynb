{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrBU0iIQEvZo"
      },
      "source": [
        "https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBxtL4GAIew4",
        "outputId": "f110fda0-6e92-42b3-de80-1f2fce49b85e"
      },
      "source": [
        "#pip install tensorflow==2.2.0\r\n",
        "!pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/77/df0c0ca6f0b1a59b812d81d7737d8ea2a95d8716f9ffc1a68822531b78fb/tf_agents-0.6.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.11.0)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.19.4)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (0.1.5)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (50.3.2)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk6aD51eIfl_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOB2vHt7Ifo0",
        "outputId": "efa511b5-2b4b-4130-c6da-d9f35ec965f3"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\r\n",
        "\r\n",
        "import base64\r\n",
        "import IPython\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tf_agents.agents.dqn import dqn_agent\r\n",
        "\r\n",
        "from tf_agents.drivers import dynamic_step_driver\r\n",
        "from tf_agents.environments import suite_gym\r\n",
        "from tf_agents.environments import tf_py_environment\r\n",
        "from tf_agents.eval import metric_utils\r\n",
        "from tf_agents.metrics import tf_metrics\r\n",
        "from tf_agents.networks import q_network\r\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n",
        "from tf_agents.trajectories import trajectory\r\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecKLhv-LIfsB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2iV6nVrIfub"
      },
      "source": [
        "env = suite_gym.load('CartPole-v1')\r\n",
        "env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fceTlj0IfxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDRtvOdDIfzx"
      },
      "source": [
        "q_net = q_network.QNetwork(env.observation_spec(), \r\n",
        "                           env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2pi6cVuIf23"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x4-5qb9If5n"
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xmk5JlTIf8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkYNXAJEIf_N"
      },
      "source": [
        "train_step_counter = tf.Variable(0)\r\n",
        "\r\n",
        "agent = dqn_agent.DqnAgent(env.time_step_spec(),\r\n",
        "                           env.action_spec(),\r\n",
        "                           q_network=q_net,\r\n",
        "                           optimizer=optimizer,\r\n",
        "                           td_errors_loss_fn= \r\n",
        "                                  common.element_wise_squared_loss,\r\n",
        "                           train_step_counter=train_step_counter)\r\n",
        "\r\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBZuNYVuIgCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGx6iOnCIgFV"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\r\n",
        "    total_return = 0.0\r\n",
        "    for _ in range(num_episodes):\r\n",
        "        time_step = environment.reset()\r\n",
        "        episode_return = 0.0        \r\n",
        "        \r\n",
        "        while not time_step.is_last():\r\n",
        "            action_step = policy.action(time_step)\r\n",
        "            time_step = environment.step(action_step.action)\r\n",
        "            episode_return += time_step.reward\r\n",
        "        total_return += episode_return    \r\n",
        "    avg_return = total_return / num_episodes\r\n",
        "    return avg_return.numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVLuLZ4nIgIE"
      },
      "source": [
        "# Evaluate the agent's policy once before training.\r\n",
        "avg_return = compute_avg_return(env, agent.policy, 5)\r\n",
        "returns = [avg_return]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqraJwiyIgK_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4dn0Y7PIgNy"
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n",
        "                                data_spec=agent.collect_data_spec,                                                                \r\n",
        "                                batch_size=env.batch_size,                                                              \r\n",
        "                                max_length=100000)\r\n",
        "\r\n",
        "\r\n",
        "def collect_step(environment, policy, buffer):\r\n",
        "    time_step = environment.current_time_step()\r\n",
        "    action_step = policy.action(time_step)\r\n",
        "    next_time_step = environment.step(action_step.action)\r\n",
        "    traj = trajectory.from_transition(time_step, \r\n",
        "                                      action_step, \r\n",
        "                                      next_time_step)\r\n",
        "    # Add trajectory to the replay buffer\r\n",
        "    buffer.add_batch(traj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lilza4_tIgQr"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95XN0jFnIgTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCYKi_3bIgYt",
        "outputId": "bb1b0156-c98e-4092-8b82-f89f9a610373"
      },
      "source": [
        "collect_steps_per_iteration = 1\r\n",
        "batch_size = 64\r\n",
        "dataset = replay_buffer.as_dataset(num_parallel_calls=3, \r\n",
        "                                   sample_batch_size=batch_size, \r\n",
        "                                   num_steps=2).prefetch(3)\r\n",
        "iterator = iter(dataset)\r\n",
        "num_iterations = 20000\r\n",
        "env.reset()\r\n",
        "\r\n",
        "for _ in range(batch_size):\r\n",
        "    collect_step(env, agent.policy, replay_buffer)\r\n",
        "\r\n",
        "for _ in range(num_iterations):\r\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\r\n",
        "    for _ in range(collect_steps_per_iteration):\r\n",
        "        collect_step(env, agent.collect_policy, replay_buffer)    \r\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\r\n",
        "    experience, unused_info = next(iterator)\r\n",
        "    train_loss = agent.train(experience).loss    \r\n",
        "    step = agent.train_step_counter.numpy()    # Print loss every 200 steps.\r\n",
        "    if step % 200 == 0:\r\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))    # Evaluate agent's performance every 1000 steps.\r\n",
        "    if step % 1000 == 0:\r\n",
        "        avg_return = compute_avg_return(env, agent.policy, 5)\r\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\r\n",
        "        returns.append(avg_return)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step = 200: loss = 16.591285705566406\n",
            "step = 400: loss = 12.585597038269043\n",
            "step = 600: loss = 20.096038818359375\n",
            "step = 800: loss = 11.211697578430176\n",
            "step = 1000: loss = 66.51399230957031\n",
            "step = 1000: Average Return = 43.0\n",
            "step = 1200: loss = 144.00088500976562\n",
            "step = 1400: loss = 307.5181884765625\n",
            "step = 1600: loss = 245.3841552734375\n",
            "step = 1800: loss = 653.9501342773438\n",
            "step = 2000: loss = 288.7919616699219\n",
            "step = 2000: Average Return = 150.8000030517578\n",
            "step = 2200: loss = 803.0383911132812\n",
            "step = 2400: loss = 188.69383239746094\n",
            "step = 2600: loss = 384.85064697265625\n",
            "step = 2800: loss = 204.84268188476562\n",
            "step = 3000: loss = 193.45167541503906\n",
            "step = 3000: Average Return = 105.80000305175781\n",
            "step = 3200: loss = 183.96319580078125\n",
            "step = 3400: loss = 160.9153289794922\n",
            "step = 3600: loss = 839.531982421875\n",
            "step = 3800: loss = 465.5520324707031\n",
            "step = 4000: loss = 615.9669799804688\n",
            "step = 4000: Average Return = 413.0\n",
            "step = 4200: loss = 628.8997192382812\n",
            "step = 4400: loss = 18019.046875\n",
            "step = 4600: loss = 653.99267578125\n",
            "step = 4800: loss = 721.0272827148438\n",
            "step = 5000: loss = 891.8192138671875\n",
            "step = 5000: Average Return = 461.3999938964844\n",
            "step = 5200: loss = 607.0750732421875\n",
            "step = 5400: loss = 519.781005859375\n",
            "step = 5600: loss = 580.6509399414062\n",
            "step = 5800: loss = 1025.823974609375\n",
            "step = 6000: loss = 3238.8017578125\n",
            "step = 6000: Average Return = 406.6000061035156\n",
            "step = 6200: loss = 1020.9439697265625\n",
            "step = 6400: loss = 795.962646484375\n",
            "step = 6600: loss = 529.515625\n",
            "step = 6800: loss = 517.40478515625\n",
            "step = 7000: loss = 1302.5823974609375\n",
            "step = 7000: Average Return = 360.6000061035156\n",
            "step = 7200: loss = 1190.3626708984375\n",
            "step = 7400: loss = 1060.0679931640625\n",
            "step = 7600: loss = 2572.5537109375\n",
            "step = 7800: loss = 2338.65185546875\n",
            "step = 8000: loss = 4136.3662109375\n",
            "step = 8000: Average Return = 490.20001220703125\n",
            "step = 8200: loss = 4196.18798828125\n",
            "step = 8400: loss = 28927.40234375\n",
            "step = 8600: loss = 7232.6474609375\n",
            "step = 8800: loss = 5714.708984375\n",
            "step = 9000: loss = 5526.970703125\n",
            "step = 9000: Average Return = 500.0\n",
            "step = 9200: loss = 7267.423828125\n",
            "step = 9400: loss = 3021.20263671875\n",
            "step = 9600: loss = 2871.2958984375\n",
            "step = 9800: loss = 5547.9833984375\n",
            "step = 10000: loss = 6173.95458984375\n",
            "step = 10000: Average Return = 448.79998779296875\n",
            "step = 10200: loss = 5383.021484375\n",
            "step = 10400: loss = 3254.96728515625\n",
            "step = 10600: loss = 5269.4189453125\n",
            "step = 10800: loss = 1766.248291015625\n",
            "step = 11000: loss = 1250.7568359375\n",
            "step = 11000: Average Return = 500.0\n",
            "step = 11200: loss = 1870.5599365234375\n",
            "step = 11400: loss = 2811.1669921875\n",
            "step = 11600: loss = 5556.80419921875\n",
            "step = 11800: loss = 2869.746826171875\n",
            "step = 12000: loss = 3865.66845703125\n",
            "step = 12000: Average Return = 500.0\n",
            "step = 12200: loss = 3570.62939453125\n",
            "step = 12400: loss = 4078.2001953125\n",
            "step = 12600: loss = 2687.42578125\n",
            "step = 12800: loss = 4641.935546875\n",
            "step = 13000: loss = 18415.962890625\n",
            "step = 13000: Average Return = 500.0\n",
            "step = 13200: loss = 18728.79296875\n",
            "step = 13400: loss = 26594.234375\n",
            "step = 13600: loss = 52353.70703125\n",
            "step = 13800: loss = 802860.5625\n",
            "step = 14000: loss = 110402.8125\n",
            "step = 14000: Average Return = 500.0\n",
            "step = 14200: loss = 69940.0625\n",
            "step = 14400: loss = 21102.12890625\n",
            "step = 14600: loss = 50169.6796875\n",
            "step = 14800: loss = 18973.98828125\n",
            "step = 15000: loss = 708680.25\n",
            "step = 15000: Average Return = 500.0\n",
            "step = 15200: loss = 83387.203125\n",
            "step = 15400: loss = 80732.25\n",
            "step = 15600: loss = 14099.04296875\n",
            "step = 15800: loss = 31990.71484375\n",
            "step = 16000: loss = 50982.7265625\n",
            "step = 16000: Average Return = 500.0\n",
            "step = 16200: loss = 50001.47265625\n",
            "step = 16400: loss = 21602.046875\n",
            "step = 16600: loss = 37026.4296875\n",
            "step = 16800: loss = 103339.546875\n",
            "step = 17000: loss = 48928.3671875\n",
            "step = 17000: Average Return = 373.6000061035156\n",
            "step = 17200: loss = 68377.6171875\n",
            "step = 17400: loss = 37211.6796875\n",
            "step = 17600: loss = 59730.7265625\n",
            "step = 17800: loss = 3566874.75\n",
            "step = 18000: loss = 107576.421875\n",
            "step = 18000: Average Return = 459.6000061035156\n",
            "step = 18200: loss = 59610.8671875\n",
            "step = 18400: loss = 89254.96875\n",
            "step = 18600: loss = 107984.453125\n",
            "step = 18800: loss = 72463.890625\n",
            "step = 19000: loss = 49543.2109375\n",
            "step = 19000: Average Return = 354.79998779296875\n",
            "step = 19200: loss = 67577.046875\n",
            "step = 19400: loss = 57279.5859375\n",
            "step = 19600: loss = 37234.5546875\n",
            "step = 19800: loss = 50037.8671875\n",
            "step = 20000: loss = 93425.359375\n",
            "step = 20000: Average Return = 335.3999938964844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6bHVgQIgb_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "bTcCUxewIgel",
        "outputId": "4ef0a135-328f-4356-f03e-bc9d3093cd96"
      },
      "source": [
        "iterations = range(0, num_iterations + 1, 1000)\r\n",
        "plt.plot(iterations, returns)\r\n",
        "plt.ylabel('Average Return')\r\n",
        "plt.xlabel('Iterations')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g2Gxqugy17z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbkoCGWjy15c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIjuCh7y129"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hIKbNQQIgzV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntYx5VQgIg2M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xmPqhogIg5B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVkzrOOoIg8L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubyc8rFKIg-x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fByIiJhjIhCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h92sl1fTIhLR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUFskH3OIhOH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIOC1HjHIhRD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA3XMHDaIhUG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qDzPkT3IhW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDjhnXfZIhZ7"
      },
      "source": [
        "http://neuro-educator.com/rl1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwQdMqZE3Cm7"
      },
      "source": [
        "# [0]ライブラリのインポート\r\n",
        "import gym  #倒立振子(cartpole)の実行環境\r\n",
        "from gym import wrappers  #gymの画像保存\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoP2MJXP3Djl"
      },
      "source": [
        "\r\n",
        " \r\n",
        "# [1]Q関数を離散化して定義する関数　------------\r\n",
        "# 観測した状態を離散値にデジタル変換する\r\n",
        "def bins(clip_min, clip_max, num):\r\n",
        "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\r\n",
        " \r\n",
        "# 各値を離散値に変換\r\n",
        "def digitize_state(observation):\r\n",
        "    cart_pos, cart_v, pole_angle, pole_v = observation\r\n",
        "    digitized = [\r\n",
        "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\r\n",
        "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\r\n",
        "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\r\n",
        "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\r\n",
        "    ]\r\n",
        "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkmXFPeI3FWE"
      },
      "source": [
        "# [2]行動a(t)を求める関数 -------------------------------------\r\n",
        "def get_action(next_state, episode):\r\n",
        "           #徐々に最適行動のみをとる、ε-greedy法\r\n",
        "    epsilon = 0.5 * (1 / (episode + 1))\r\n",
        "    if epsilon <= np.random.uniform(0, 1):\r\n",
        "        next_action = np.argmax(q_table[next_state])\r\n",
        "    else:\r\n",
        "        next_action = np.random.choice([0, 1])\r\n",
        "    return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYUlQNz23FPP"
      },
      "source": [
        "\r\n",
        " \r\n",
        " \r\n",
        "# [3]Qテーブルを更新する関数 -------------------------------------\r\n",
        "def update_Qtable(q_table, state, action, reward, next_state):\r\n",
        "    gamma = 0.99\r\n",
        "    alpha = 0.5\r\n",
        "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\r\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\r\n",
        "            alpha * (reward + gamma * next_Max_Q)\r\n",
        "   \r\n",
        "    return q_table\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWR6Lr5t3IYU"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# [4]. メイン関数開始 パラメータ設定--------------------------------------------------------\r\n",
        "env = gym.make('CartPole-v0')\r\n",
        "max_number_of_steps = 200  #1試行のstep数\r\n",
        "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\r\n",
        "num_episodes = 2000  #総試行回数\r\n",
        "goal_average_reward = 195  #この報酬を超えると学習終了（中心への制御なし）\r\n",
        "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\r\n",
        "num_dizitized = 6  #分割数\r\n",
        "q_table = np.random.uniform(\r\n",
        "    low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\r\n",
        " \r\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\r\n",
        "final_x = np.zeros((num_episodes, 1))  #学習後、各試行のt=200でのｘの位置を格納\r\n",
        "islearned = 0  #学習が終わったフラグ\r\n",
        "isrender = 0  #描画フラグ\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSd9c-1R3IR_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0cEtsstHIh0K",
        "outputId": "a8990ecf-2742-45a0-9ca0-ce997d8ed08c"
      },
      "source": [
        "\r\n",
        " \r\n",
        " \r\n",
        " \r\n",
        "# [5] メインルーチン--------------------------------------------------\r\n",
        "for episode in range(num_episodes):  #試行数分繰り返す\r\n",
        "    # 環境の初期化\r\n",
        "    observation = env.reset()\r\n",
        "    state = digitize_state(observation)\r\n",
        "    action = np.argmax(q_table[state])\r\n",
        "    episode_reward = 0\r\n",
        " \r\n",
        "    for t in range(max_number_of_steps):  #1試行のループ\r\n",
        "        if islearned == 1:  #学習終了したらcartPoleを描画する\r\n",
        "            env.render()\r\n",
        "            time.sleep(0.1)\r\n",
        "            print (observation[0])  #カートのx位置を出力\r\n",
        " \r\n",
        "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\r\n",
        "        observation, reward, done, info = env.step(action)\r\n",
        " \r\n",
        "        # 報酬を設定し与える\r\n",
        "        if done:\r\n",
        "            if t < 195:\r\n",
        "                reward = -200  #こけたら罰則\r\n",
        "            else:\r\n",
        "                reward = 1  #立ったまま終了時は罰則はなし\r\n",
        "        else:\r\n",
        "            reward = 1  #各ステップで立ってたら報酬追加\r\n",
        " \r\n",
        "        episode_reward += reward  #報酬を追加\r\n",
        " \r\n",
        "        # 離散状態s_{t+1}を求め、Q関数を更新する\r\n",
        "        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\r\n",
        "        q_table = update_Qtable(q_table, state, action, reward, next_state)\r\n",
        "        \r\n",
        "        #  次の行動a_{t+1}を求める \r\n",
        "        action = get_action(next_state, episode)    # a_{t+1} \r\n",
        "        \r\n",
        "        state = next_state\r\n",
        "        \r\n",
        "        #終了時の処理\r\n",
        "        if done:\r\n",
        "            print('%d Episode finished after %f time steps / mean %f' %\r\n",
        "                  (episode, t + 1, total_reward_vec.mean()))\r\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:],\r\n",
        "                                          episode_reward))  #報酬を記録\r\n",
        "            if islearned == 1:  #学習終わってたら最終のx座標を格納\r\n",
        "                final_x[episode, 0] = observation[0]\r\n",
        "            break\r\n",
        " \r\n",
        "    if (total_reward_vec.mean() >=\r\n",
        "            goal_average_reward):  # 直近の100エピソードが規定報酬以上であれば成功\r\n",
        "        print('Episode %d train agent successfuly!' % episode)\r\n",
        "        islearned = 1\r\n",
        "        #np.savetxt('learned_Q_table.csv',q_table, delimiter=\",\") #Qtableの保存する場合\r\n",
        "        if isrender == 0:\r\n",
        "            #env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\r\n",
        "            isrender = 1\r\n",
        "    #10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\r\n",
        "    #if episode>10:\r\n",
        "    #    if isrender == 0:\r\n",
        "    #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\r\n",
        "    #        isrender = 1\r\n",
        "    #    islearned=1;\r\n",
        " \r\n",
        "if islearned:\r\n",
        "    np.savetxt('final_x.csv', final_x, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Episode finished after 16.000000 time steps / mean 0.000000\n",
            "1 Episode finished after 17.000000 time steps / mean -1.850000\n",
            "2 Episode finished after 55.000000 time steps / mean -3.690000\n",
            "3 Episode finished after 38.000000 time steps / mean -5.150000\n",
            "4 Episode finished after 11.000000 time steps / mean -6.780000\n",
            "5 Episode finished after 42.000000 time steps / mean -8.680000\n",
            "6 Episode finished after 29.000000 time steps / mean -10.270000\n",
            "7 Episode finished after 10.000000 time steps / mean -11.990000\n",
            "8 Episode finished after 33.000000 time steps / mean -13.900000\n",
            "9 Episode finished after 9.000000 time steps / mean -15.580000\n",
            "10 Episode finished after 25.000000 time steps / mean -17.500000\n",
            "11 Episode finished after 11.000000 time steps / mean -19.260000\n",
            "12 Episode finished after 15.000000 time steps / mean -21.160000\n",
            "13 Episode finished after 10.000000 time steps / mean -23.020000\n",
            "14 Episode finished after 27.000000 time steps / mean -24.930000\n",
            "15 Episode finished after 11.000000 time steps / mean -26.670000\n",
            "16 Episode finished after 55.000000 time steps / mean -28.570000\n",
            "17 Episode finished after 50.000000 time steps / mean -30.030000\n",
            "18 Episode finished after 37.000000 time steps / mean -31.540000\n",
            "19 Episode finished after 28.000000 time steps / mean -33.180000\n",
            "20 Episode finished after 24.000000 time steps / mean -34.910000\n",
            "21 Episode finished after 120.000000 time steps / mean -36.680000\n",
            "22 Episode finished after 45.000000 time steps / mean -37.490000\n",
            "23 Episode finished after 59.000000 time steps / mean -39.050000\n",
            "24 Episode finished after 9.000000 time steps / mean -40.470000\n",
            "25 Episode finished after 49.000000 time steps / mean -42.390000\n",
            "26 Episode finished after 28.000000 time steps / mean -43.910000\n",
            "27 Episode finished after 10.000000 time steps / mean -45.640000\n",
            "28 Episode finished after 45.000000 time steps / mean -47.550000\n",
            "29 Episode finished after 150.000000 time steps / mean -49.110000\n",
            "30 Episode finished after 40.000000 time steps / mean -49.620000\n",
            "31 Episode finished after 10.000000 time steps / mean -51.230000\n",
            "32 Episode finished after 120.000000 time steps / mean -53.140000\n",
            "33 Episode finished after 133.000000 time steps / mean -53.950000\n",
            "34 Episode finished after 191.000000 time steps / mean -54.630000\n",
            "35 Episode finished after 200.000000 time steps / mean -54.730000\n",
            "36 Episode finished after 147.000000 time steps / mean -52.730000\n",
            "37 Episode finished after 122.000000 time steps / mean -53.270000\n",
            "38 Episode finished after 45.000000 time steps / mean -54.060000\n",
            "39 Episode finished after 25.000000 time steps / mean -55.620000\n",
            "40 Episode finished after 41.000000 time steps / mean -57.380000\n",
            "41 Episode finished after 200.000000 time steps / mean -58.980000\n",
            "42 Episode finished after 30.000000 time steps / mean -56.980000\n",
            "43 Episode finished after 31.000000 time steps / mean -58.690000\n",
            "44 Episode finished after 200.000000 time steps / mean -60.390000\n",
            "45 Episode finished after 178.000000 time steps / mean -58.390000\n",
            "46 Episode finished after 179.000000 time steps / mean -58.620000\n",
            "47 Episode finished after 137.000000 time steps / mean -58.840000\n",
            "48 Episode finished after 141.000000 time steps / mean -59.480000\n",
            "49 Episode finished after 195.000000 time steps / mean -60.080000\n",
            "50 Episode finished after 200.000000 time steps / mean -60.140000\n",
            "51 Episode finished after 170.000000 time steps / mean -58.140000\n",
            "52 Episode finished after 200.000000 time steps / mean -58.450000\n",
            "53 Episode finished after 193.000000 time steps / mean -56.450000\n",
            "54 Episode finished after 139.000000 time steps / mean -56.530000\n",
            "55 Episode finished after 38.000000 time steps / mean -57.150000\n",
            "56 Episode finished after 101.000000 time steps / mean -58.780000\n",
            "57 Episode finished after 78.000000 time steps / mean -59.780000\n",
            "58 Episode finished after 30.000000 time steps / mean -61.010000\n",
            "59 Episode finished after 175.000000 time steps / mean -62.720000\n",
            "60 Episode finished after 200.000000 time steps / mean -62.980000\n",
            "61 Episode finished after 151.000000 time steps / mean -60.980000\n",
            "62 Episode finished after 200.000000 time steps / mean -61.480000\n",
            "63 Episode finished after 190.000000 time steps / mean -59.480000\n",
            "64 Episode finished after 184.000000 time steps / mean -59.590000\n",
            "65 Episode finished after 181.000000 time steps / mean -59.760000\n",
            "66 Episode finished after 158.000000 time steps / mean -59.960000\n",
            "67 Episode finished after 141.000000 time steps / mean -60.390000\n",
            "68 Episode finished after 174.000000 time steps / mean -60.990000\n",
            "69 Episode finished after 139.000000 time steps / mean -61.260000\n",
            "70 Episode finished after 97.000000 time steps / mean -61.880000\n",
            "71 Episode finished after 126.000000 time steps / mean -62.920000\n",
            "72 Episode finished after 190.000000 time steps / mean -63.670000\n",
            "73 Episode finished after 126.000000 time steps / mean -63.780000\n",
            "74 Episode finished after 76.000000 time steps / mean -64.530000\n",
            "75 Episode finished after 78.000000 time steps / mean -65.780000\n",
            "76 Episode finished after 76.000000 time steps / mean -67.010000\n",
            "77 Episode finished after 42.000000 time steps / mean -68.260000\n",
            "78 Episode finished after 102.000000 time steps / mean -69.850000\n",
            "79 Episode finished after 100.000000 time steps / mean -70.840000\n",
            "80 Episode finished after 140.000000 time steps / mean -71.850000\n",
            "81 Episode finished after 72.000000 time steps / mean -72.460000\n",
            "82 Episode finished after 200.000000 time steps / mean -73.750000\n",
            "83 Episode finished after 110.000000 time steps / mean -71.750000\n",
            "84 Episode finished after 200.000000 time steps / mean -72.660000\n",
            "85 Episode finished after 79.000000 time steps / mean -70.660000\n",
            "86 Episode finished after 117.000000 time steps / mean -71.880000\n",
            "87 Episode finished after 114.000000 time steps / mean -72.720000\n",
            "88 Episode finished after 200.000000 time steps / mean -73.590000\n",
            "89 Episode finished after 166.000000 time steps / mean -71.590000\n",
            "90 Episode finished after 200.000000 time steps / mean -71.940000\n",
            "91 Episode finished after 148.000000 time steps / mean -69.940000\n",
            "92 Episode finished after 200.000000 time steps / mean -70.470000\n",
            "93 Episode finished after 200.000000 time steps / mean -68.470000\n",
            "94 Episode finished after 195.000000 time steps / mean -66.470000\n",
            "95 Episode finished after 142.000000 time steps / mean -66.530000\n",
            "96 Episode finished after 200.000000 time steps / mean -67.120000\n",
            "97 Episode finished after 177.000000 time steps / mean -65.120000\n",
            "98 Episode finished after 64.000000 time steps / mean -65.360000\n",
            "99 Episode finished after 70.000000 time steps / mean -66.730000\n",
            "100 Episode finished after 62.000000 time steps / mean -68.040000\n",
            "101 Episode finished after 200.000000 time steps / mean -67.580000\n",
            "102 Episode finished after 200.000000 time steps / mean -63.740000\n",
            "103 Episode finished after 199.000000 time steps / mean -60.280000\n",
            "104 Episode finished after 156.000000 time steps / mean -56.660000\n",
            "105 Episode finished after 186.000000 time steps / mean -55.210000\n",
            "106 Episode finished after 200.000000 time steps / mean -53.770000\n",
            "107 Episode finished after 145.000000 time steps / mean -50.050000\n",
            "108 Episode finished after 159.000000 time steps / mean -48.700000\n",
            "109 Episode finished after 154.000000 time steps / mean -47.440000\n",
            "110 Episode finished after 124.000000 time steps / mean -45.990000\n",
            "111 Episode finished after 200.000000 time steps / mean -45.000000\n",
            "112 Episode finished after 140.000000 time steps / mean -41.100000\n",
            "113 Episode finished after 200.000000 time steps / mean -39.850000\n",
            "114 Episode finished after 157.000000 time steps / mean -35.940000\n",
            "115 Episode finished after 111.000000 time steps / mean -34.640000\n",
            "116 Episode finished after 200.000000 time steps / mean -33.640000\n",
            "117 Episode finished after 200.000000 time steps / mean -30.180000\n",
            "118 Episode finished after 200.000000 time steps / mean -26.670000\n",
            "119 Episode finished after 200.000000 time steps / mean -23.030000\n",
            "120 Episode finished after 200.000000 time steps / mean -19.300000\n",
            "121 Episode finished after 200.000000 time steps / mean -15.530000\n",
            "122 Episode finished after 194.000000 time steps / mean -12.720000\n",
            "123 Episode finished after 200.000000 time steps / mean -11.230000\n",
            "124 Episode finished after 172.000000 time steps / mean -7.810000\n",
            "125 Episode finished after 163.000000 time steps / mean -6.180000\n",
            "126 Episode finished after 45.000000 time steps / mean -5.040000\n",
            "127 Episode finished after 200.000000 time steps / mean -4.870000\n",
            "128 Episode finished after 166.000000 time steps / mean -0.960000\n",
            "129 Episode finished after 200.000000 time steps / mean 0.250000\n",
            "130 Episode finished after 200.000000 time steps / mean 2.760000\n",
            "131 Episode finished after 134.000000 time steps / mean 6.370000\n",
            "132 Episode finished after 200.000000 time steps / mean 7.610000\n",
            "133 Episode finished after 200.000000 time steps / mean 10.420000\n",
            "134 Episode finished after 200.000000 time steps / mean 13.100000\n",
            "135 Episode finished after 200.000000 time steps / mean 15.200000\n",
            "136 Episode finished after 200.000000 time steps / mean 15.200000\n",
            "137 Episode finished after 166.000000 time steps / mean 17.740000\n",
            "138 Episode finished after 200.000000 time steps / mean 18.180000\n",
            "139 Episode finished after 164.000000 time steps / mean 21.740000\n",
            "140 Episode finished after 200.000000 time steps / mean 23.130000\n",
            "141 Episode finished after 200.000000 time steps / mean 26.730000\n",
            "142 Episode finished after 200.000000 time steps / mean 26.730000\n",
            "143 Episode finished after 200.000000 time steps / mean 30.440000\n",
            "144 Episode finished after 186.000000 time steps / mean 34.140000\n",
            "145 Episode finished after 200.000000 time steps / mean 31.990000\n",
            "146 Episode finished after 195.000000 time steps / mean 34.220000\n",
            "147 Episode finished after 187.000000 time steps / mean 34.380000\n",
            "148 Episode finished after 200.000000 time steps / mean 34.880000\n",
            "149 Episode finished after 153.000000 time steps / mean 37.480000\n",
            "150 Episode finished after 150.000000 time steps / mean 37.060000\n",
            "151 Episode finished after 200.000000 time steps / mean 34.550000\n",
            "152 Episode finished after 200.000000 time steps / mean 36.860000\n",
            "153 Episode finished after 185.000000 time steps / mean 36.860000\n",
            "154 Episode finished after 152.000000 time steps / mean 36.780000\n",
            "155 Episode finished after 200.000000 time steps / mean 36.910000\n",
            "156 Episode finished after 200.000000 time steps / mean 40.540000\n",
            "157 Episode finished after 200.000000 time steps / mean 43.540000\n",
            "158 Episode finished after 200.000000 time steps / mean 46.770000\n",
            "159 Episode finished after 80.000000 time steps / mean 50.480000\n",
            "160 Episode finished after 192.000000 time steps / mean 49.530000\n",
            "161 Episode finished after 200.000000 time steps / mean 47.440000\n",
            "162 Episode finished after 200.000000 time steps / mean 49.940000\n",
            "163 Episode finished after 200.000000 time steps / mean 49.940000\n",
            "164 Episode finished after 194.000000 time steps / mean 52.050000\n",
            "165 Episode finished after 200.000000 time steps / mean 52.150000\n",
            "166 Episode finished after 200.000000 time steps / mean 54.350000\n",
            "167 Episode finished after 200.000000 time steps / mean 56.780000\n",
            "168 Episode finished after 155.000000 time steps / mean 59.380000\n",
            "169 Episode finished after 200.000000 time steps / mean 59.190000\n",
            "170 Episode finished after 200.000000 time steps / mean 61.810000\n",
            "171 Episode finished after 200.000000 time steps / mean 64.850000\n",
            "172 Episode finished after 135.000000 time steps / mean 67.600000\n",
            "173 Episode finished after 200.000000 time steps / mean 67.050000\n",
            "174 Episode finished after 200.000000 time steps / mean 69.800000\n",
            "175 Episode finished after 142.000000 time steps / mean 73.050000\n",
            "176 Episode finished after 127.000000 time steps / mean 73.690000\n",
            "177 Episode finished after 200.000000 time steps / mean 74.200000\n",
            "178 Episode finished after 160.000000 time steps / mean 77.790000\n",
            "179 Episode finished after 197.000000 time steps / mean 78.370000\n",
            "180 Episode finished after 199.000000 time steps / mean 81.350000\n",
            "181 Episode finished after 190.000000 time steps / mean 83.950000\n",
            "182 Episode finished after 181.000000 time steps / mean 85.130000\n",
            "183 Episode finished after 84.000000 time steps / mean 82.930000\n",
            "184 Episode finished after 174.000000 time steps / mean 82.670000\n",
            "185 Episode finished after 200.000000 time steps / mean 80.400000\n",
            "186 Episode finished after 200.000000 time steps / mean 83.620000\n",
            "187 Episode finished after 178.000000 time steps / mean 86.460000\n",
            "188 Episode finished after 200.000000 time steps / mean 87.100000\n",
            "189 Episode finished after 200.000000 time steps / mean 87.100000\n",
            "190 Episode finished after 200.000000 time steps / mean 89.450000\n",
            "191 Episode finished after 200.000000 time steps / mean 89.450000\n",
            "192 Episode finished after 172.000000 time steps / mean 91.980000\n",
            "193 Episode finished after 141.000000 time steps / mean 89.690000\n",
            "194 Episode finished after 66.000000 time steps / mean 87.090000\n",
            "195 Episode finished after 20.000000 time steps / mean 85.800000\n",
            "196 Episode finished after 200.000000 time steps / mean 84.580000\n",
            "197 Episode finished after 200.000000 time steps / mean 84.580000\n",
            "198 Episode finished after 200.000000 time steps / mean 86.820000\n",
            "199 Episode finished after 136.000000 time steps / mean 90.190000\n",
            "200 Episode finished after 161.000000 time steps / mean 90.850000\n",
            "201 Episode finished after 200.000000 time steps / mean 91.840000\n",
            "202 Episode finished after 163.000000 time steps / mean 91.840000\n",
            "203 Episode finished after 200.000000 time steps / mean 89.460000\n",
            "204 Episode finished after 200.000000 time steps / mean 89.470000\n",
            "205 Episode finished after 200.000000 time steps / mean 91.920000\n",
            "206 Episode finished after 130.000000 time steps / mean 94.070000\n",
            "207 Episode finished after 183.000000 time steps / mean 91.360000\n",
            "208 Episode finished after 184.000000 time steps / mean 91.740000\n",
            "209 Episode finished after 200.000000 time steps / mean 91.990000\n",
            "210 Episode finished after 200.000000 time steps / mean 94.460000\n",
            "211 Episode finished after 200.000000 time steps / mean 97.230000\n",
            "212 Episode finished after 200.000000 time steps / mean 97.230000\n",
            "213 Episode finished after 200.000000 time steps / mean 99.840000\n",
            "214 Episode finished after 100.000000 time steps / mean 99.840000\n",
            "215 Episode finished after 58.000000 time steps / mean 99.270000\n",
            "216 Episode finished after 36.000000 time steps / mean 98.740000\n",
            "217 Episode finished after 88.000000 time steps / mean 95.090000\n",
            "218 Episode finished after 47.000000 time steps / mean 91.960000\n",
            "219 Episode finished after 200.000000 time steps / mean 88.420000\n",
            "220 Episode finished after 200.000000 time steps / mean 88.420000\n",
            "221 Episode finished after 200.000000 time steps / mean 88.420000\n",
            "222 Episode finished after 185.000000 time steps / mean 88.420000\n",
            "223 Episode finished after 179.000000 time steps / mean 88.330000\n",
            "224 Episode finished after 155.000000 time steps / mean 86.110000\n",
            "225 Episode finished after 122.000000 time steps / mean 85.940000\n",
            "226 Episode finished after 179.000000 time steps / mean 85.530000\n",
            "227 Episode finished after 200.000000 time steps / mean 86.870000\n",
            "228 Episode finished after 200.000000 time steps / mean 86.870000\n",
            "229 Episode finished after 200.000000 time steps / mean 89.220000\n",
            "230 Episode finished after 199.000000 time steps / mean 89.220000\n",
            "231 Episode finished after 200.000000 time steps / mean 89.210000\n",
            "232 Episode finished after 200.000000 time steps / mean 91.880000\n",
            "233 Episode finished after 200.000000 time steps / mean 91.880000\n",
            "234 Episode finished after 200.000000 time steps / mean 91.880000\n",
            "235 Episode finished after 126.000000 time steps / mean 91.880000\n",
            "236 Episode finished after 200.000000 time steps / mean 89.130000\n",
            "237 Episode finished after 200.000000 time steps / mean 89.130000\n",
            "238 Episode finished after 200.000000 time steps / mean 91.480000\n",
            "239 Episode finished after 200.000000 time steps / mean 91.480000\n",
            "240 Episode finished after 130.000000 time steps / mean 93.850000\n",
            "241 Episode finished after 200.000000 time steps / mean 91.140000\n",
            "242 Episode finished after 200.000000 time steps / mean 91.140000\n",
            "243 Episode finished after 156.000000 time steps / mean 91.140000\n",
            "244 Episode finished after 180.000000 time steps / mean 88.690000\n",
            "245 Episode finished after 200.000000 time steps / mean 88.630000\n",
            "246 Episode finished after 200.000000 time steps / mean 88.630000\n",
            "247 Episode finished after 200.000000 time steps / mean 90.690000\n",
            "248 Episode finished after 200.000000 time steps / mean 92.830000\n",
            "249 Episode finished after 200.000000 time steps / mean 92.830000\n",
            "250 Episode finished after 200.000000 time steps / mean 95.310000\n",
            "251 Episode finished after 200.000000 time steps / mean 97.820000\n",
            "252 Episode finished after 187.000000 time steps / mean 97.820000\n",
            "253 Episode finished after 152.000000 time steps / mean 95.680000\n",
            "254 Episode finished after 150.000000 time steps / mean 95.350000\n",
            "255 Episode finished after 145.000000 time steps / mean 95.330000\n",
            "256 Episode finished after 28.000000 time steps / mean 92.770000\n",
            "257 Episode finished after 126.000000 time steps / mean 89.040000\n",
            "258 Episode finished after 183.000000 time steps / mean 86.290000\n",
            "259 Episode finished after 154.000000 time steps / mean 84.110000\n",
            "260 Episode finished after 165.000000 time steps / mean 84.850000\n",
            "261 Episode finished after 189.000000 time steps / mean 84.580000\n",
            "262 Episode finished after 200.000000 time steps / mean 82.460000\n",
            "263 Episode finished after 200.000000 time steps / mean 82.460000\n",
            "264 Episode finished after 200.000000 time steps / mean 82.460000\n",
            "265 Episode finished after 200.000000 time steps / mean 84.530000\n",
            "266 Episode finished after 200.000000 time steps / mean 84.530000\n",
            "267 Episode finished after 167.000000 time steps / mean 84.530000\n",
            "268 Episode finished after 200.000000 time steps / mean 82.190000\n",
            "269 Episode finished after 168.000000 time steps / mean 84.650000\n",
            "270 Episode finished after 200.000000 time steps / mean 82.320000\n",
            "271 Episode finished after 200.000000 time steps / mean 82.320000\n",
            "272 Episode finished after 200.000000 time steps / mean 82.320000\n",
            "273 Episode finished after 200.000000 time steps / mean 84.980000\n",
            "274 Episode finished after 167.000000 time steps / mean 84.980000\n",
            "275 Episode finished after 200.000000 time steps / mean 82.640000\n",
            "276 Episode finished after 167.000000 time steps / mean 85.230000\n",
            "277 Episode finished after 189.000000 time steps / mean 85.630000\n",
            "278 Episode finished after 200.000000 time steps / mean 83.510000\n",
            "279 Episode finished after 200.000000 time steps / mean 85.920000\n",
            "280 Episode finished after 200.000000 time steps / mean 85.950000\n",
            "281 Episode finished after 200.000000 time steps / mean 85.960000\n",
            "282 Episode finished after 200.000000 time steps / mean 88.070000\n",
            "283 Episode finished after 188.000000 time steps / mean 90.270000\n",
            "284 Episode finished after 172.000000 time steps / mean 91.310000\n",
            "285 Episode finished after 193.000000 time steps / mean 91.290000\n",
            "286 Episode finished after 191.000000 time steps / mean 89.210000\n",
            "287 Episode finished after 200.000000 time steps / mean 87.110000\n",
            "288 Episode finished after 194.000000 time steps / mean 89.340000\n",
            "289 Episode finished after 200.000000 time steps / mean 87.270000\n",
            "290 Episode finished after 130.000000 time steps / mean 87.270000\n",
            "291 Episode finished after 200.000000 time steps / mean 84.560000\n",
            "292 Episode finished after 160.000000 time steps / mean 84.560000\n",
            "293 Episode finished after 200.000000 time steps / mean 84.440000\n",
            "294 Episode finished after 200.000000 time steps / mean 87.040000\n",
            "295 Episode finished after 155.000000 time steps / mean 90.390000\n",
            "296 Episode finished after 200.000000 time steps / mean 91.740000\n",
            "297 Episode finished after 144.000000 time steps / mean 91.740000\n",
            "298 Episode finished after 183.000000 time steps / mean 89.170000\n",
            "299 Episode finished after 189.000000 time steps / mean 86.990000\n",
            "300 Episode finished after 168.000000 time steps / mean 87.520000\n",
            "301 Episode finished after 192.000000 time steps / mean 87.590000\n",
            "302 Episode finished after 200.000000 time steps / mean 85.500000\n",
            "303 Episode finished after 200.000000 time steps / mean 87.880000\n",
            "304 Episode finished after 200.000000 time steps / mean 87.880000\n",
            "305 Episode finished after 200.000000 time steps / mean 87.880000\n",
            "306 Episode finished after 200.000000 time steps / mean 87.880000\n",
            "307 Episode finished after 200.000000 time steps / mean 90.590000\n",
            "308 Episode finished after 200.000000 time steps / mean 92.770000\n",
            "309 Episode finished after 83.000000 time steps / mean 94.940000\n",
            "310 Episode finished after 188.000000 time steps / mean 91.760000\n",
            "311 Episode finished after 170.000000 time steps / mean 89.630000\n",
            "312 Episode finished after 197.000000 time steps / mean 87.320000\n",
            "313 Episode finished after 200.000000 time steps / mean 87.290000\n",
            "314 Episode finished after 200.000000 time steps / mean 87.290000\n",
            "315 Episode finished after 200.000000 time steps / mean 90.300000\n",
            "316 Episode finished after 169.000000 time steps / mean 93.730000\n",
            "317 Episode finished after 200.000000 time steps / mean 95.060000\n",
            "318 Episode finished after 200.000000 time steps / mean 98.190000\n",
            "319 Episode finished after 200.000000 time steps / mean 101.730000\n",
            "320 Episode finished after 200.000000 time steps / mean 101.730000\n",
            "321 Episode finished after 161.000000 time steps / mean 101.730000\n",
            "322 Episode finished after 198.000000 time steps / mean 99.330000\n",
            "323 Episode finished after 168.000000 time steps / mean 101.470000\n",
            "324 Episode finished after 200.000000 time steps / mean 101.360000\n",
            "325 Episode finished after 187.000000 time steps / mean 103.820000\n",
            "326 Episode finished after 180.000000 time steps / mean 104.470000\n",
            "327 Episode finished after 199.000000 time steps / mean 104.480000\n",
            "328 Episode finished after 150.000000 time steps / mean 104.470000\n",
            "329 Episode finished after 200.000000 time steps / mean 101.960000\n",
            "330 Episode finished after 200.000000 time steps / mean 101.960000\n",
            "331 Episode finished after 160.000000 time steps / mean 101.970000\n",
            "332 Episode finished after 171.000000 time steps / mean 99.560000\n",
            "333 Episode finished after 180.000000 time steps / mean 97.260000\n",
            "334 Episode finished after 152.000000 time steps / mean 95.050000\n",
            "335 Episode finished after 193.000000 time steps / mean 92.560000\n",
            "336 Episode finished after 171.000000 time steps / mean 93.230000\n",
            "337 Episode finished after 186.000000 time steps / mean 90.930000\n",
            "338 Episode finished after 37.000000 time steps / mean 88.780000\n",
            "339 Episode finished after 200.000000 time steps / mean 85.140000\n",
            "340 Episode finished after 188.000000 time steps / mean 85.140000\n",
            "341 Episode finished after 142.000000 time steps / mean 85.720000\n",
            "342 Episode finished after 76.000000 time steps / mean 83.130000\n",
            "343 Episode finished after 194.000000 time steps / mean 79.880000\n",
            "344 Episode finished after 139.000000 time steps / mean 80.260000\n",
            "345 Episode finished after 200.000000 time steps / mean 79.850000\n",
            "346 Episode finished after 123.000000 time steps / mean 79.850000\n",
            "347 Episode finished after 131.000000 time steps / mean 77.070000\n",
            "348 Episode finished after 95.000000 time steps / mean 74.370000\n",
            "349 Episode finished after 179.000000 time steps / mean 71.310000\n",
            "350 Episode finished after 200.000000 time steps / mean 69.090000\n",
            "351 Episode finished after 159.000000 time steps / mean 69.090000\n",
            "352 Episode finished after 77.000000 time steps / mean 66.670000\n",
            "353 Episode finished after 165.000000 time steps / mean 65.570000\n",
            "354 Episode finished after 154.000000 time steps / mean 65.700000\n",
            "355 Episode finished after 193.000000 time steps / mean 65.740000\n",
            "356 Episode finished after 142.000000 time steps / mean 66.220000\n",
            "357 Episode finished after 15.000000 time steps / mean 67.360000\n",
            "358 Episode finished after 27.000000 time steps / mean 66.250000\n",
            "359 Episode finished after 21.000000 time steps / mean 64.690000\n",
            "360 Episode finished after 16.000000 time steps / mean 63.360000\n",
            "361 Episode finished after 87.000000 time steps / mean 61.870000\n",
            "362 Episode finished after 25.000000 time steps / mean 60.850000\n",
            "363 Episode finished after 65.000000 time steps / mean 57.090000\n",
            "364 Episode finished after 140.000000 time steps / mean 53.730000\n",
            "365 Episode finished after 183.000000 time steps / mean 51.120000\n",
            "366 Episode finished after 177.000000 time steps / mean 48.940000\n",
            "367 Episode finished after 179.000000 time steps / mean 46.700000\n",
            "368 Episode finished after 174.000000 time steps / mean 46.820000\n",
            "369 Episode finished after 197.000000 time steps / mean 44.550000\n",
            "370 Episode finished after 199.000000 time steps / mean 46.850000\n",
            "371 Episode finished after 173.000000 time steps / mean 46.840000\n",
            "372 Episode finished after 128.000000 time steps / mean 44.560000\n",
            "373 Episode finished after 137.000000 time steps / mean 41.830000\n",
            "374 Episode finished after 83.000000 time steps / mean 39.190000\n",
            "375 Episode finished after 126.000000 time steps / mean 38.350000\n",
            "376 Episode finished after 126.000000 time steps / mean 35.600000\n",
            "377 Episode finished after 119.000000 time steps / mean 35.190000\n",
            "378 Episode finished after 126.000000 time steps / mean 34.490000\n",
            "379 Episode finished after 200.000000 time steps / mean 31.740000\n",
            "380 Episode finished after 200.000000 time steps / mean 31.740000\n",
            "381 Episode finished after 200.000000 time steps / mean 31.740000\n",
            "382 Episode finished after 200.000000 time steps / mean 31.740000\n",
            "383 Episode finished after 200.000000 time steps / mean 31.740000\n",
            "384 Episode finished after 200.000000 time steps / mean 33.870000\n",
            "385 Episode finished after 200.000000 time steps / mean 36.160000\n",
            "386 Episode finished after 200.000000 time steps / mean 38.240000\n",
            "387 Episode finished after 85.000000 time steps / mean 40.340000\n",
            "388 Episode finished after 200.000000 time steps / mean 37.180000\n",
            "389 Episode finished after 200.000000 time steps / mean 39.250000\n",
            "390 Episode finished after 200.000000 time steps / mean 39.250000\n",
            "391 Episode finished after 200.000000 time steps / mean 41.960000\n",
            "392 Episode finished after 200.000000 time steps / mean 41.960000\n",
            "393 Episode finished after 200.000000 time steps / mean 44.370000\n",
            "394 Episode finished after 200.000000 time steps / mean 44.370000\n",
            "395 Episode finished after 200.000000 time steps / mean 44.370000\n",
            "396 Episode finished after 200.000000 time steps / mean 46.830000\n",
            "397 Episode finished after 200.000000 time steps / mean 46.830000\n",
            "398 Episode finished after 200.000000 time steps / mean 49.400000\n",
            "399 Episode finished after 200.000000 time steps / mean 51.580000\n",
            "400 Episode finished after 200.000000 time steps / mean 53.700000\n",
            "401 Episode finished after 200.000000 time steps / mean 56.030000\n",
            "402 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "403 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "404 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "405 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "406 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "407 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "408 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "409 Episode finished after 200.000000 time steps / mean 58.120000\n",
            "410 Episode finished after 200.000000 time steps / mean 61.300000\n",
            "411 Episode finished after 200.000000 time steps / mean 63.430000\n",
            "412 Episode finished after 200.000000 time steps / mean 65.740000\n",
            "413 Episode finished after 200.000000 time steps / mean 65.770000\n",
            "414 Episode finished after 200.000000 time steps / mean 65.770000\n",
            "415 Episode finished after 200.000000 time steps / mean 65.770000\n",
            "416 Episode finished after 200.000000 time steps / mean 65.770000\n",
            "417 Episode finished after 200.000000 time steps / mean 68.090000\n",
            "418 Episode finished after 200.000000 time steps / mean 68.090000\n",
            "419 Episode finished after 200.000000 time steps / mean 68.090000\n",
            "420 Episode finished after 200.000000 time steps / mean 68.090000\n",
            "421 Episode finished after 200.000000 time steps / mean 68.090000\n",
            "422 Episode finished after 200.000000 time steps / mean 70.490000\n",
            "423 Episode finished after 200.000000 time steps / mean 70.510000\n",
            "424 Episode finished after 200.000000 time steps / mean 72.840000\n",
            "425 Episode finished after 200.000000 time steps / mean 72.840000\n",
            "426 Episode finished after 200.000000 time steps / mean 74.980000\n",
            "427 Episode finished after 200.000000 time steps / mean 77.190000\n",
            "428 Episode finished after 200.000000 time steps / mean 77.200000\n",
            "429 Episode finished after 200.000000 time steps / mean 79.710000\n",
            "430 Episode finished after 200.000000 time steps / mean 79.710000\n",
            "431 Episode finished after 200.000000 time steps / mean 79.710000\n",
            "432 Episode finished after 200.000000 time steps / mean 82.120000\n",
            "433 Episode finished after 200.000000 time steps / mean 84.420000\n",
            "434 Episode finished after 200.000000 time steps / mean 86.630000\n",
            "435 Episode finished after 200.000000 time steps / mean 89.120000\n",
            "436 Episode finished after 200.000000 time steps / mean 91.200000\n",
            "437 Episode finished after 200.000000 time steps / mean 93.500000\n",
            "438 Episode finished after 200.000000 time steps / mean 95.650000\n",
            "439 Episode finished after 200.000000 time steps / mean 99.290000\n",
            "440 Episode finished after 200.000000 time steps / mean 99.290000\n",
            "441 Episode finished after 200.000000 time steps / mean 101.420000\n",
            "442 Episode finished after 200.000000 time steps / mean 104.010000\n",
            "443 Episode finished after 200.000000 time steps / mean 107.260000\n",
            "444 Episode finished after 200.000000 time steps / mean 109.330000\n",
            "445 Episode finished after 200.000000 time steps / mean 111.950000\n",
            "446 Episode finished after 200.000000 time steps / mean 111.950000\n",
            "447 Episode finished after 200.000000 time steps / mean 114.730000\n",
            "448 Episode finished after 200.000000 time steps / mean 117.430000\n",
            "449 Episode finished after 200.000000 time steps / mean 120.490000\n",
            "450 Episode finished after 16.000000 time steps / mean 122.710000\n",
            "451 Episode finished after 200.000000 time steps / mean 118.860000\n",
            "452 Episode finished after 200.000000 time steps / mean 121.280000\n",
            "453 Episode finished after 200.000000 time steps / mean 124.520000\n",
            "454 Episode finished after 200.000000 time steps / mean 126.880000\n",
            "455 Episode finished after 200.000000 time steps / mean 129.350000\n",
            "456 Episode finished after 200.000000 time steps / mean 131.430000\n",
            "457 Episode finished after 200.000000 time steps / mean 134.020000\n",
            "458 Episode finished after 200.000000 time steps / mean 137.880000\n",
            "459 Episode finished after 200.000000 time steps / mean 141.620000\n",
            "460 Episode finished after 200.000000 time steps / mean 145.420000\n",
            "461 Episode finished after 200.000000 time steps / mean 149.270000\n",
            "462 Episode finished after 200.000000 time steps / mean 152.410000\n",
            "463 Episode finished after 200.000000 time steps / mean 156.170000\n",
            "464 Episode finished after 200.000000 time steps / mean 159.530000\n",
            "465 Episode finished after 200.000000 time steps / mean 162.140000\n",
            "466 Episode finished after 200.000000 time steps / mean 164.320000\n",
            "467 Episode finished after 200.000000 time steps / mean 166.560000\n",
            "468 Episode finished after 200.000000 time steps / mean 168.780000\n",
            "469 Episode finished after 200.000000 time steps / mean 171.050000\n",
            "470 Episode finished after 200.000000 time steps / mean 171.080000\n",
            "471 Episode finished after 200.000000 time steps / mean 171.090000\n",
            "472 Episode finished after 200.000000 time steps / mean 173.370000\n",
            "473 Episode finished after 200.000000 time steps / mean 176.100000\n",
            "474 Episode finished after 200.000000 time steps / mean 178.740000\n",
            "475 Episode finished after 200.000000 time steps / mean 181.920000\n",
            "476 Episode finished after 200.000000 time steps / mean 184.670000\n",
            "477 Episode finished after 200.000000 time steps / mean 187.420000\n",
            "478 Episode finished after 200.000000 time steps / mean 190.240000\n",
            "479 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "480 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "481 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "482 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "483 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "484 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "485 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "486 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "487 Episode finished after 200.000000 time steps / mean 192.990000\n",
            "Episode 487 train agent successfuly!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-9396d1110461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number_of_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#1試行のループ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mislearned\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#学習終了したらcartPoleを描画する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#カートのx位置を出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEYGjemXIh3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDBuOB2B3Uqu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngV7uzCMIh53"
      },
      "source": [
        "https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEQgA08wIh8U"
      },
      "source": [
        "http://neuro-educator.com/rl2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KJKLBdhIh-0"
      },
      "source": [
        "# [0]必要なライブラリのインポート\r\n",
        "import gym  # 倒立振子(cartpole)の実行環境\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.utils import plot_model\r\n",
        "from collections import deque\r\n",
        "from gym import wrappers  # gymの画像保存\r\n",
        "from keras import backend as K\r\n",
        " \r\n",
        "# [1]損失関数の定義\r\n",
        "# 損失関数に関数を使用します\r\n",
        "def huberloss(y_true, y_pred):\r\n",
        "    return K.mean(K.minimum(0.5*K.square(y_pred-y_true), K.abs(y_pred-y_true)-0.5), axis=1)\r\n",
        " \r\n",
        " \r\n",
        "# [2]Q関数をディープラーニングのネットワークをクラスとして定義\r\n",
        "class QNetwork:\r\n",
        "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10):\r\n",
        "        self.model = Sequential()\r\n",
        "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size))\r\n",
        "        self.model.add(Dense(hidden_size, activation='relu'))\r\n",
        "        self.model.add(Dense(action_size, activation='linear'))\r\n",
        "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\r\n",
        "        # self.model.compile(loss='mse', optimizer=self.optimizer)\r\n",
        "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\r\n",
        " \r\n",
        "    # 重みの学習\r\n",
        "    def replay(self, memory, batch_size, gamma, targetQN):\r\n",
        "        inputs = np.zeros((batch_size, 4))\r\n",
        "        targets = np.zeros((batch_size, 2))\r\n",
        "        mini_batch = memory.sample(batch_size)\r\n",
        " \r\n",
        "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\r\n",
        "            inputs[i:i + 1] = state_b\r\n",
        "            target = reward_b\r\n",
        " \r\n",
        "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\r\n",
        "                # 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）\r\n",
        "                retmainQs = self.model.predict(next_state_b)[0]\r\n",
        "                next_action = np.argmax(retmainQs)  # 最大の報酬を返す行動を選択する\r\n",
        "                target = reward_b + gamma * targetQN.model.predict(next_state_b)[0][next_action]\r\n",
        "                \r\n",
        "            targets[i] = self.model.predict(state_b)    # Qネットワークの出力\r\n",
        "            targets[i][action_b] = target               # 教師信号\r\n",
        "            self.model.fit(inputs, targets, epochs=1, verbose=0)  # epochsは訓練データの反復回数、verbose=0は表示なしの設定\r\n",
        " \r\n",
        " \r\n",
        "# [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス\r\n",
        "class Memory:\r\n",
        "    def __init__(self, max_size=1000):\r\n",
        "        self.buffer = deque(maxlen=max_size)\r\n",
        " \r\n",
        "    def add(self, experience):\r\n",
        "        self.buffer.append(experience)\r\n",
        " \r\n",
        "    def sample(self, batch_size):\r\n",
        "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\r\n",
        "        return [self.buffer[ii] for ii in idx]\r\n",
        " \r\n",
        "    def len(self):\r\n",
        "        return len(self.buffer)\r\n",
        " \r\n",
        " \r\n",
        "# [4]カートの状態に応じて、行動を決定するクラス\r\n",
        "class Actor:\r\n",
        "    def get_action(self, state, episode, targetQN):   # [C]ｔ＋１での行動を返す\r\n",
        "        # 徐々に最適行動のみをとる、ε-greedy法\r\n",
        "        epsilon = 0.001 + 0.9 / (1.0+episode)\r\n",
        " \r\n",
        "        if epsilon <= np.random.uniform(0, 1):\r\n",
        "            retTargetQs = targetQN.model.predict(state)[0]\r\n",
        "            action = np.argmax(retTargetQs)  # 最大の報酬を返す行動を選択する\r\n",
        " \r\n",
        "        else:\r\n",
        "            action = np.random.choice([0, 1])  # ランダムに行動する\r\n",
        " \r\n",
        "        return action\r\n",
        " \r\n",
        " \r\n",
        "# [5] メイン関数開始----------------------------------------------------\r\n",
        "# [5.1] 初期設定--------------------------------------------------------\r\n",
        "DQN_MODE = 1    # 1がDQN、0がDDQNです\r\n",
        "LENDER_MODE = 1 # 0は学習後も描画なし、1は学習終了後に描画する\r\n",
        " \r\n",
        "env = gym.make('CartPole-v0')\r\n",
        "num_episodes = 299  # 総試行回数\r\n",
        "max_number_of_steps = 200  # 1試行のstep数\r\n",
        "goal_average_reward = 195  # この報酬を超えると学習終了\r\n",
        "num_consecutive_iterations = 10  # 学習完了評価の平均計算を行う試行回数\r\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\r\n",
        "gamma = 0.99    # 割引係数\r\n",
        "islearned = 0  # 学習が終わったフラグ\r\n",
        "isrender = 0  # 描画フラグ\r\n",
        "# ---\r\n",
        "hidden_size = 16               # Q-networkの隠れ層のニューロンの数\r\n",
        "learning_rate = 0.00001         # Q-networkの学習係数\r\n",
        "memory_size = 10000            # バッファーメモリの大きさ\r\n",
        "batch_size = 32                # Q-networkを更新するバッチの大記載\r\n",
        " \r\n",
        "# [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------\r\n",
        "mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)     # メインのQネットワーク\r\n",
        "targetQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)   # 価値を計算するQネットワーク\r\n",
        "# plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化\r\n",
        "memory = Memory(max_size=memory_size)\r\n",
        "actor = Actor()\r\n",
        " \r\n",
        "# [5.3]メインルーチン--------------------------------------------------------\r\n",
        "for episode in range(num_episodes):  # 試行数分繰り返す\r\n",
        "    env.reset()  # cartPoleの環境初期化\r\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())  # 1step目は適当な行動をとる\r\n",
        "    state = np.reshape(state, [1, 4])   # list型のstateを、1行4列の行列に変換\r\n",
        "    episode_reward = 0\r\n",
        " \r\n",
        "    targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\r\n",
        " \r\n",
        "    for t in range(max_number_of_steps + 1):  # 1試行のループ\r\n",
        "        if (islearned == 1) and LENDER_MODE:  # 学習終了したらcartPoleを描画する\r\n",
        "            env.render()\r\n",
        "            time.sleep(0.1)\r\n",
        "            print(state[0, 0])  # カートのx位置を出力するならコメントはずす\r\n",
        " \r\n",
        "        action = actor.get_action(state, episode, mainQN)   # 時刻tでの行動を決定する\r\n",
        "        next_state, reward, done, info = env.step(action)   # 行動a_tの実行による、s_{t+1}, _R{t}を計算する\r\n",
        "        next_state = np.reshape(next_state, [1, 4])     # list型のstateを、1行4列の行列に変換\r\n",
        " \r\n",
        "        # 報酬を設定し、与える\r\n",
        "        if done:\r\n",
        "            next_state = np.zeros(state.shape)  # 次の状態s_{t+1}はない\r\n",
        "            if t < 195:\r\n",
        "                reward = -1  # 報酬クリッピング、報酬は1, 0, -1に固定\r\n",
        "            else:\r\n",
        "                reward = 1  # 立ったまま195step超えて終了時は報酬\r\n",
        "        else:\r\n",
        "            reward = 0  # 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）\r\n",
        " \r\n",
        "        episode_reward += 1 # reward  # 合計報酬を更新\r\n",
        " \r\n",
        "        memory.add((state, action, reward, next_state))     # メモリの更新する\r\n",
        "        state = next_state  # 状態更新\r\n",
        " \r\n",
        " \r\n",
        "        # Qネットワークの重みを学習・更新する replay\r\n",
        "        if (memory.len() > batch_size) and not islearned:\r\n",
        "            mainQN.replay(memory, batch_size, gamma, targetQN)\r\n",
        " \r\n",
        "        if DQN_MODE:\r\n",
        "            targetQN = mainQN  # 行動決定と価値計算のQネットワークをおなじにする\r\n",
        " \r\n",
        "        # 1施行終了時の処理\r\n",
        "        if done:\r\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\r\n",
        "            print('%d Episode finished after %f time steps / mean %f' % (episode, t + 1, total_reward_vec.mean()))\r\n",
        "            break\r\n",
        " \r\n",
        "    # 複数施行の平均報酬で終了を判断\r\n",
        "    if total_reward_vec.mean() >= goal_average_reward:\r\n",
        "        print('Episode %d train agent successfuly!' % episode)\r\n",
        "        islearned = 1\r\n",
        "        if isrender == 0:   # 学習済みフラグを更新\r\n",
        "            isrender = 1\r\n",
        " \r\n",
        "            # env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合\r\n",
        "            # 10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\r\n",
        "            # if episode>10:\r\n",
        "            #    if isrender == 0:\r\n",
        "            #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\r\n",
        "            #        isrender = 1\r\n",
        "            #    islearned=1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUDpJvoFIiBm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxxzlrcMIiES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um8qJLa9IiG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8UGIn3NIiJq"
      },
      "source": [
        "https://ailog.site/2019/10/25/torch7/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvsRnUU9IiMX"
      },
      "source": [
        "https://deepage.net/features/numpy-rl.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKXXctToIiPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKcRa9G_IiR0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRhygZ2SIiUx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0GD3nNIiXb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V5hfETjIiam"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJdqy2G7Iidz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_MApLBjIigz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvTWNxruIijW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdvr1psVIimt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WndczGsJIioy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y1SXg_FIirH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}